import pickle, os
import numpy as np
import pdb
from copy import deepcopy
import zarr
import shutil
import argparse
import yaml
import cv2
import h5py
try:
    import torch
    import clip
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    print("Warning: CLIP not available, using dummy text features")
from typing import List

# æ·»åŠ UVAæ¨¡å—è·¯å¾„
sys.path.append('/home/zijian/RoboTwin/policy/UVA')
from unified_video_action.model.common.rotation_transformer import RotationTransformer


def load_embodiment_config(embodiment_type):
    """åŠ è½½embodimenté…ç½®æ–‡ä»¶"""
    # ä½¿ç”¨ç»å¯¹è·¯å¾„æ¥é¿å…ç›¸å¯¹è·¯å¾„é—®é¢˜
    current_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(current_dir, "../../task_config/_embodiment_config.yml")
    
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Embodiment config file not found: {config_path}")
    
    with open(config_path, "r", encoding="utf-8") as f:
        embodiment_configs = yaml.safe_load(f)
    
    if embodiment_type not in embodiment_configs:
        raise ValueError(f"Unknown embodiment type: {embodiment_type}")
    
    robot_file = embodiment_configs[embodiment_type]["file_path"]
    
    # å¤„ç†ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆä¿®å¤è·¯å¾„å†—ä½™é—®é¢˜ï¼‰
    if not os.path.isabs(robot_file):
        # ç›´æ¥ä½¿ç”¨os.path.normpathæ¥æ¸…ç†è·¯å¾„
        robot_file = os.path.normpath(os.path.join(current_dir, "../../", robot_file))
    
    robot_config_path = os.path.join(robot_file, "config.yml")
    
    if not os.path.exists(robot_config_path):
        raise FileNotFoundError(f"Robot config file not found: {robot_config_path}")
    
    with open(robot_config_path, "r", encoding="utf-8") as f:
        robot_config = yaml.safe_load(f)
    
    return robot_config


def analyze_joint_data(joint_data, name="joint_data"):
    """åˆ†æå…³èŠ‚æ•°æ®çš„ç‰¹å¾ï¼ˆç²¾ç®€ç‰ˆï¼‰"""
    # åªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    if os.environ.get('DEBUG_JOINT_DATA') == '1':
        print(f"        ğŸ” åˆ†æ{name}ç‰¹å¾:")
        print(f"          å½¢çŠ¶: {joint_data.shape}")
        print(f"          æ•°æ®ç±»å‹: {joint_data.dtype}")
        print(f"          æ•°å€¼èŒƒå›´: [{joint_data.min():.4f}, {joint_data.max():.4f}]")
        print(f"          å‡å€¼: {joint_data.mean():.4f}")
        print(f"          æ ‡å‡†å·®: {joint_data.std():.4f}")
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å…³èŠ‚è§’åº¦èŒƒå›´å†…
        if joint_data.min() >= -np.pi and joint_data.max() <= np.pi:
            print(f"          âœ… æ•°æ®åœ¨å…³èŠ‚è§’åº¦èŒƒå›´å†… [-Ï€, Ï€]")
        elif joint_data.min() >= -2*np.pi and joint_data.max() <= 2*np.pi:
            print(f"          âš ï¸  æ•°æ®åœ¨æ‰©å±•å…³èŠ‚è§’åº¦èŒƒå›´å†… [-2Ï€, 2Ï€]")
        else:
            print(f"          â“ æ•°æ®è¶…å‡ºæ ‡å‡†å…³èŠ‚è§’åº¦èŒƒå›´")
        
        # æ£€æŸ¥æ¯ä¸ªç»´åº¦çš„ç‰¹å¾
        for i in range(min(3, joint_data.shape[-1])):  # åªæ˜¾ç¤ºå‰3ä¸ªç»´åº¦
            dim_data = joint_data[..., i]
            print(f"          ç»´åº¦{i}: èŒƒå›´[{dim_data.min():.4f}, {dim_data.max():.4f}], å‡å€¼{dim_data.mean():.4f}")
        
        if joint_data.shape[-1] > 3:
            print(f"          ... è¿˜æœ‰{joint_data.shape[-1]-3}ä¸ªç»´åº¦")
    else:
        # ç²¾ç®€ç‰ˆï¼šåªæ˜¾ç¤ºå…³é”®ä¿¡æ¯
        print(f"        {name}: {joint_data.shape} {joint_data.dtype}")


def detect_rotation_format(data):
    """æ£€æµ‹æ—‹è½¬æ•°æ®çš„è¡¨ç¤ºæ ¼å¼"""
    if data.shape[-1] == 4:
        return "quaternion"
    elif data.shape[-1] == 6:
        return "rotation_6d"
    elif data.shape[-1] == 3:
        return "axis_angle"
    elif data.shape[-1] == 7:
        # 7ç»´æ•°æ®å¯èƒ½æ˜¯å…³èŠ‚è§’åº¦ + å¤¹çˆªï¼Œæˆ–è€…æ˜¯7å…³èŠ‚æœºå™¨äºº
        # æ£€æŸ¥æ•°æ®èŒƒå›´æ¥åˆ¤æ–­
        data_min, data_max = data.min(), data.max()
        if data_min >= -np.pi and data_max <= np.pi:
            # å¦‚æœæ•°æ®åœ¨[-Ï€, Ï€]èŒƒå›´å†…ï¼Œå¾ˆå¯èƒ½æ˜¯å…³èŠ‚è§’åº¦
            return "joint_angles"
        else:
            # å¦‚æœè¶…å‡ºè¿™ä¸ªèŒƒå›´ï¼Œå¯èƒ½æ˜¯å…¶ä»–æ ¼å¼
            return "joint_angles_extended"
    elif data.shape[-1] > 10:
        # é«˜ç»´æ•°æ®å¯èƒ½æ˜¯ç»„åˆçŠ¶æ€å‘é‡
        return "state_vector"
    else:
        # å¯¹äºå…¶ä»–ç»´åº¦ï¼Œå°è¯•åˆ†ææ•°æ®ç‰¹å¾
        data_min, data_max = data.min(), data.max()
        data_std = data.std()
        
        print(f"      ğŸ” æœªçŸ¥ç»´åº¦æ•°æ®ç‰¹å¾: å½¢çŠ¶={data.shape}, èŒƒå›´=[{data_min:.3f}, {data_max:.3f}], æ ‡å‡†å·®={data_std:.3f}")
        
        # æ ¹æ®æ•°æ®ç‰¹å¾æ¨æµ‹æ ¼å¼
        if data_std < 0.1:
            return "constant_or_small_variation"
        elif abs(data_min) < 1 and abs(data_max) < 1:
            return "normalized_data"
        else:
            return "raw_joint_data"


def verify_data_integrity(original_data, processed_data, tolerance=1e-6):
    """éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼Œæ£€æŸ¥æ˜¯å¦å‘ç”Ÿæ„å¤–ä¿®æ”¹"""
    print(f"    ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
    
    # æ£€æŸ¥æ•°æ®ç±»å‹
    if hasattr(original_data, 'dtype') and hasattr(processed_data, 'dtype'):
        if original_data.dtype != processed_data.dtype:
            print(f"      âš ï¸  æ•°æ®ç±»å‹å˜åŒ–: {original_data.dtype} â†’ {processed_data.dtype}")
    
    # æ£€æŸ¥æ•°æ®å½¢çŠ¶
    if hasattr(original_data, 'shape') and hasattr(processed_data, 'shape'):
        if original_data.shape != processed_data.shape:
            print(f"      âš ï¸  æ•°æ®å½¢çŠ¶å˜åŒ–: {original_data.shape} â†’ {processed_data.shape}")
    
    # æ£€æŸ¥æ•°å€¼èŒƒå›´
    if hasattr(original_data, 'min') and hasattr(original_data, 'max'):
        orig_min, orig_max = original_data.min(), original_data.max()
        proc_min, proc_max = processed_data.min(), processed_data.max()
        
        if abs(orig_min - proc_min) > tolerance or abs(orig_max - proc_max) > tolerance:
            print(f"      âš ï¸  æ•°å€¼èŒƒå›´å˜åŒ–: [{orig_min:.6f}, {orig_max:.6f}] â†’ [{proc_min:.6f}, {proc_max:.6f}]")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–æ— ç©·å¤§å€¼
    if hasattr(processed_data, 'dtype') and np.issubdtype(processed_data.dtype, np.floating):
        if np.any(np.isnan(processed_data)):
            print(f"      âŒ æ£€æµ‹åˆ°NaNå€¼ï¼")
        if np.any(np.isinf(processed_data)):
            print(f"      âŒ æ£€æµ‹åˆ°æ— ç©·å¤§å€¼ï¼")
    
    print(f"      âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯å®Œæˆ")


def normalize_rotation_data(rotation_data, source_format, target_format="rotation_6d"):
    """å°†æ—‹è½¬æ•°æ®ç»Ÿä¸€è½¬æ¢ä¸ºç›®æ ‡æ ¼å¼ï¼ŒåŒ…å«æ•°æ®å®Œæ•´æ€§éªŒè¯"""
    if source_format == target_format:
        return rotation_data
    
    # ä¿å­˜åŸå§‹æ•°æ®çš„å‰¯æœ¬ç”¨äºéªŒè¯
    original_data = rotation_data.copy()
    
    print(f"      ğŸ”„ è½¬æ¢æ—‹è½¬æ•°æ®: {source_format} â†’ {target_format}")
    
    # ç‰¹æ®Šå¤„ç†å…³èŠ‚è§’åº¦æ•°æ®
    if source_format in ["joint_angles", "joint_angles_extended", "raw_joint_data"]:
        print(f"        â„¹ï¸  æ£€æµ‹åˆ°å…³èŠ‚è§’åº¦æ•°æ®ï¼Œç»´åº¦: {rotation_data.shape[-1]}")
        
        # å¯¹äºå…³èŠ‚è§’åº¦æ•°æ®ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªé€‰æ‹©ï¼š
        # 1. ä¿æŒåŸå§‹æ ¼å¼ï¼ˆæ¨èï¼‰
        # 2. å°è¯•è½¬æ¢ä¸ºrotation_6dï¼ˆå¦‚æœç»´åº¦åˆé€‚ï¼‰
        
        if rotation_data.shape[-1] == 7:
            print(f"        â„¹ï¸  7ç»´å…³èŠ‚æ•°æ®ï¼Œä¿æŒåŸå§‹æ ¼å¼ä»¥ç¡®ä¿æ•°æ®å®Œæ•´æ€§")
            return rotation_data  # ä¿æŒåŸå§‹æ ¼å¼
        
        elif rotation_data.shape[-1] == 6:
            print(f"        â„¹ï¸  6ç»´æ•°æ®ï¼Œå°è¯•è½¬æ¢ä¸ºrotation_6d")
            # 6ç»´æ•°æ®å¯èƒ½æ˜¯å‰6ä¸ªå…³èŠ‚ï¼Œå¯ä»¥å°è¯•è½¬æ¢
            try:
                # å‡è®¾å‰6ç»´æ˜¯æ—‹è½¬å…³èŠ‚ï¼Œè½¬æ¢ä¸ºrotation_6d
                transformer = RotationTransformer("axis_angle", "rotation_6d")
                # è¿™é‡Œéœ€è¦å°†å…³èŠ‚è§’åº¦è½¬æ¢ä¸ºè½´è§’è¡¨ç¤º
                # ç®€åŒ–å¤„ç†ï¼šå‡è®¾æ¯ä¸ªå…³èŠ‚éƒ½æ˜¯ç»•Zè½´çš„æ—‹è½¬
                converted_data = rotation_data.copy()
                return converted_data
            except Exception as e:
                print(f"        âš ï¸  è½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸå§‹æ ¼å¼: {e}")
                return rotation_data
        
        else:
            print(f"        â„¹ï¸  {rotation_data.shape[-1]}ç»´å…³èŠ‚æ•°æ®ï¼Œä¿æŒåŸå§‹æ ¼å¼")
            return rotation_data
    
    try:
        # ç¬¬ä¸€å±‚fallbackï¼šç›´æ¥è½¬æ¢
        transformer = RotationTransformer(source_format, target_format)
        converted_data = transformer.forward(rotation_data)
        
        # éªŒè¯è½¬æ¢ç»“æœ
        verify_data_integrity(original_data, converted_data)
        
        return converted_data
    except Exception as e:
        print(f"        âš ï¸  ç›´æ¥è½¬æ¢å¤±è´¥: {e}")
        
        # ç¬¬äºŒå±‚fallbackï¼šé€šè¿‡axis_angleä½œä¸ºä¸­é—´æ ¼å¼
        if source_format != "axis_angle" and target_format != "axis_angle":
            try:
                print(f"        ğŸ”„ å°è¯•é€šè¿‡axis_angleè½¬æ¢...")
                transformer1 = RotationTransformer(source_format, "axis_angle")
                transformer2 = RotationTransformer("axis_angle", target_format)
                intermediate = transformer1.forward(rotation_data)
                converted_data = transformer2.forward(intermediate)
                
                # éªŒè¯è½¬æ¢ç»“æœ
                verify_data_integrity(original_data, converted_data)
                
                return converted_data
            except Exception as e2:
                print(f"        âš ï¸  é€šè¿‡axis_angleè½¬æ¢ä¹Ÿå¤±è´¥: {e2}")
                
                # ç¬¬ä¸‰å±‚fallbackï¼šè¿”å›åŸå§‹æ•°æ®
                print(f"        âš ï¸  è¿”å›åŸå§‹æ•°æ®ï¼Œæœªè¿›è¡Œè½¬æ¢")
                return rotation_data


def extract_endpose_data(hdf5_data, embodiment_config):
    """ä»HDF5æ•°æ®ä¸­æå–endposeä¿¡æ¯"""
    try:
        # å°è¯•ä»embodimenté…ç½®ä¸­è·å–æœ«ç«¯æ‰§è¡Œå™¨ä¿¡æ¯
        if "ee_joints" in embodiment_config:
            left_ee_joint = embodiment_config["ee_joints"][0]
            right_ee_joint = embodiment_config["ee_joints"][1]
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¯¹åº”çš„è§‚å¯Ÿæ•°æ®
            if f"/observation/{left_ee_joint}" in hdf5_data:
                left_ee_state = hdf5_data[f"/observation/{left_ee_joint}"][()]
                right_ee_state = hdf5_data[f"/observation/{right_ee_joint}"][()]
                
                return {
                    "left_endpose": left_ee_state,
                    "right_endpose": right_ee_state
                }
    except Exception as e:
        print(f"Warning: Failed to extract endpose data: {e}")
    
    return None


def load_hdf5(dataset_path, embodiment_config=None):
    """åŠ è½½HDF5æ•°æ®ï¼Œæ”¯æŒä¸åŒæœ¬ä½“æ ¼å¼"""
    if not os.path.isfile(dataset_path):
        print(f"âŒ Dataset does not exist at {dataset_path}")
        exit()

    with h5py.File(dataset_path, "r") as root:
        # å°è¯•ä¸åŒçš„æ•°æ®æ ¼å¼
        data = {}
        
        # ç²¾ç®€è¾“å‡ºï¼šåªæ˜¾ç¤ºå…³é”®ç»“æ„ä¿¡æ¯
        if os.environ.get('DEBUG_HDF5') == '1':
            print(f"    HDF5æ–‡ä»¶ç»“æ„: {list(root.keys())}")
        
        # æ£€æŸ¥joint_actionç»“æ„
        if "/joint_action" in root:
            joint_action = root["/joint_action"]
            if os.environ.get('DEBUG_HDF5') == '1':
                print(f"    Joint actionç»“æ„: {list(joint_action.keys())}")
            
            # å°è¯•åŠ è½½åŒè‡‚æ•°æ® - åˆ›å»ºå‰¯æœ¬ä¿æŠ¤åŸå§‹æ•°æ®
            if "left_gripper" in joint_action and "left_arm" in joint_action:
                data["left_gripper"] = joint_action["left_gripper"][()].copy()  # åˆ›å»ºå‰¯æœ¬
                data["left_arm"] = joint_action["left_arm"][()].copy()  # åˆ›å»ºå‰¯æœ¬
                data["right_gripper"] = joint_action["right_gripper"][()].copy()  # åˆ›å»ºå‰¯æœ¬
                data["right_arm"] = joint_action["right_arm"][()].copy()  # åˆ›å»ºå‰¯æœ¬
                data["format"] = "dual_arm_separate"
                print(f"    âœ“ åŒè‡‚åˆ†ç¦»æ ¼å¼: å·¦è‡‚{data['left_arm'].shape}, å³è‡‚{data['right_arm'].shape}")
                
                # åˆ†æå…³èŠ‚æ•°æ®ç‰¹å¾
                analyze_joint_data(data["left_arm"], "å·¦è‡‚")
                analyze_joint_data(data["right_arm"], "å³è‡‚")
                analyze_joint_data(data["left_gripper"], "å·¦å¤¹çˆª")
                analyze_joint_data(data["right_gripper"], "å³å¤¹çˆª")
                
            elif "left_arm" in joint_action and "right_arm" in joint_action:
                data["left_arm"] = joint_action["left_arm"][()].copy()  # åˆ›å»ºå‰¯æœ¬
                data["right_arm"] = joint_action["right_arm"][()].copy()  # åˆ›å»ºå‰¯æœ¬
                data["format"] = "dual_arm_no_gripper"
                print(f"    âœ“ åŒè‡‚æ— å¤¹çˆªæ ¼å¼: å·¦è‡‚{data['left_arm'].shape}, å³è‡‚{data['right_arm'].shape}")
                
                # åˆ†æå…³èŠ‚æ•°æ®ç‰¹å¾
                analyze_joint_data(data["left_arm"], "å·¦è‡‚")
                analyze_joint_data(data["right_arm"], "å³è‡‚")
            else:
                # å°è¯•åŠ è½½å•è‡‚æ•°æ®
                for key in joint_action.keys():
                    if "arm" in key or "gripper" in key:
                        data[key] = joint_action[key][()].copy()  # åˆ›å»ºå‰¯æœ¬
                        print(f"    âœ“ å•è‡‚æ•°æ®: {key} {data[key].shape}")
                data["format"] = "single_arm"
        
        # æ£€æŸ¥vectoræ•°æ®
        if "/joint_action/vector" in root:
            data["vector"] = root["/joint_action/vector"][()].copy()  # åˆ›å»ºå‰¯æœ¬
            print(f"    âœ“ Vectoræ•°æ®: {data['vector'].shape}")
        
        # åŠ è½½å›¾åƒæ•°æ®
        image_dict = dict()
        if "/observation" in root:
            if os.environ.get('DEBUG_HDF5') == '1':
                print(f"    Observationç»“æ„: {list(root['/observation'].keys())}")
            for cam_name in root["/observation"].keys():
                if "rgb" in root[f"/observation/{cam_name}"]:
                    image_dict[cam_name] = root[f"/observation/{cam_name}/rgb"][()].copy()  # åˆ›å»ºå‰¯æœ¬
                    if os.environ.get('DEBUG_HDF5') == '1':
                        print(f"    âœ“ ç›¸æœº: {cam_name} {image_dict[cam_name].shape}")
        
        data["image_dict"] = image_dict
        
        # å°è¯•æå–endposeæ•°æ®
        if embodiment_config:
            endpose_data = extract_endpose_data(root, embodiment_config)
            if endpose_data:
                data["endpose"] = endpose_data
                if os.environ.get('DEBUG_HDF5') == '1':
                    print(f"    âœ“ æˆåŠŸæå–endposeæ•°æ®")

    return data


def normalize_robot_data_safe(data, embodiment_type):
    """å®‰å…¨åœ°æ ‡å‡†åŒ–æœºå™¨äººæ•°æ®ï¼Œä¿æŒåŸå§‹ç»´åº¦å’Œç‰©ç†æ„ä¹‰"""
    print(f"      ğŸ”§ æ ‡å‡†åŒ– {embodiment_type} æ•°æ®...")
    
    normalized_data = {}
    
    if data["format"] == "dual_arm_separate":
        print(f"        âœ“ åŒè‡‚åˆ†ç¦»æ ¼å¼")
        
        # ä¿æŒåŸå§‹ç»´åº¦ï¼Œä¸è¿›è¡Œå¼ºåˆ¶è½¬æ¢
        left_arm = data["left_arm"].copy()
        right_arm = data["right_arm"].copy()
        left_gripper = data["left_gripper"].copy()
        right_gripper = data["right_gripper"].copy()
        
        print(f"        å·¦è‡‚: {left_arm.shape}, å³è‡‚: {right_arm.shape}")
        print(f"        å·¦å¤¹çˆª: {left_gripper.shape}, å³å¤¹çˆª: {right_gripper.shape}")
        
        # åˆ†ææ•°æ®ç‰¹å¾ï¼Œä½†ä¸å¼ºåˆ¶ä¿®æ”¹
        analyze_joint_data(left_arm, "å·¦è‡‚")
        analyze_joint_data(right_arm, "å³è‡‚")
        analyze_joint_data(left_gripper, "å·¦å¤¹çˆª")
        analyze_joint_data(right_gripper, "å³å¤¹çˆª")
        
        # åˆ›å»ºactionæ•°ç»„ï¼Œä¿æŒåŸå§‹ç»´åº¦
        timesteps = left_arm.shape[0]
        
        # è®¡ç®—æ€»actionç»´åº¦
        left_arm_dim = left_arm.shape[1] if left_arm.ndim > 1 else 1
        right_arm_dim = right_arm.shape[1] if right_arm.ndim > 1 else 1
        left_gripper_dim = left_gripper.shape[1] if left_gripper.ndim > 1 else 1
        right_gripper_dim = right_gripper.shape[1] if right_gripper.ndim > 1 else 1
        
        total_action_dim = left_arm_dim + right_arm_dim + left_gripper_dim + right_gripper_dim
        
        print(f"        æ€»actionç»´åº¦: {total_action_dim}")
        
        # åˆ›å»ºactionæ•°ç»„
        normalized_action = np.zeros((timesteps, total_action_dim))
        
        # å¡«å……æ•°æ®ï¼Œä¿æŒåŸå§‹ç»“æ„
        start_idx = 0
        
        # å·¦è‡‚
        if left_arm.ndim > 1:
            normalized_action[:, start_idx:start_idx+left_arm_dim] = left_arm
        else:
            normalized_action[:, start_idx] = left_arm
        start_idx += left_arm_dim
        
        # å³è‡‚
        if right_arm.ndim > 1:
            normalized_action[:, start_idx:start_idx+right_arm_dim] = right_arm
        else:
            normalized_action[:, start_idx] = right_arm
        start_idx += right_arm_dim
        
        # å·¦å¤¹çˆª
        if left_gripper.ndim > 1:
            normalized_action[:, start_idx:start_idx+left_gripper_dim] = left_gripper
        else:
            normalized_action[:, start_idx] = left_gripper
        start_idx += left_gripper_dim
        
        # å³å¤¹çˆª
        if right_gripper.ndim > 1:
            normalized_action[:, start_idx:start_idx+right_gripper_dim] = right_gripper
        else:
            normalized_action[:, start_idx] = right_gripper
        
        normalized_data["action"] = normalized_action
        normalized_data["action_dim"] = total_action_dim
        normalized_data["action_structure"] = {
            "left_arm": {"start": 0, "end": left_arm_dim, "dim": left_arm_dim},
            "right_arm": {"start": left_arm_dim, "end": left_arm_dim + right_arm_dim, "dim": right_arm_dim},
            "left_gripper": {"start": left_arm_dim + right_arm_dim, "end": left_arm_dim + right_arm_dim + left_gripper_dim, "dim": left_gripper_dim},
            "right_gripper": {"start": left_arm_dim + right_arm_dim + left_gripper_dim, "end": total_action_dim, "dim": right_gripper_dim}
        }
        
    elif data["format"] == "dual_arm_no_gripper":
        print(f"        âœ“ åŒè‡‚æ— å¤¹çˆªæ ¼å¼")
        
        left_arm = data["left_arm"].copy()
        right_arm = data["right_arm"].copy()
        
        print(f"        å·¦è‡‚: {left_arm.shape}, å³è‡‚: {right_arm.shape}")
        
        # åˆ†ææ•°æ®ç‰¹å¾
        analyze_joint_data(left_arm, "å·¦è‡‚")
        analyze_joint_data(right_arm, "å³è‡‚")
        
        # åˆ›å»ºactionæ•°ç»„
        timesteps = left_arm.shape[0]
        left_arm_dim = left_arm.shape[1] if left_arm.ndim > 1 else 1
        right_arm_dim = right_arm.shape[1] if right_arm.ndim > 1 else 1
        
        total_action_dim = left_arm_dim + right_arm_dim
        
        normalized_action = np.zeros((timesteps, total_action_dim))
        
        start_idx = 0
        if left_arm.ndim > 1:
            normalized_action[:, start_idx:start_idx+left_arm_dim] = left_arm
        else:
            normalized_action[:, start_idx] = left_arm
        start_idx += left_arm_dim
        
        if right_arm.ndim > 1:
            normalized_action[:, start_idx:start_idx+right_arm_dim] = right_arm
        else:
            normalized_action[:, start_idx] = right_arm
        
        normalized_data["action"] = normalized_action
        normalized_data["action_dim"] = total_action_dim
        normalized_data["action_structure"] = {
            "left_arm": {"start": 0, "end": left_arm_dim, "dim": left_arm_dim},
            "right_arm": {"start": left_arm_dim, "end": total_action_dim, "dim": right_arm_dim}
        }
        
    else:
        print(f"        âœ“ å•è‡‚æ ¼å¼")
        
        # å¤„ç†å•è‡‚æ•°æ®
        action_parts = []
        action_structure = {}
        start_idx = 0
        
        for key, value in data.items():
            if "arm" in key or "gripper" in key:
                action_parts.append(value.copy())
                part_dim = value.shape[1] if value.ndim > 1 else 1
                action_structure[key] = {"start": start_idx, "end": start_idx + part_dim, "dim": part_dim}
                start_idx += part_dim
                print(f"        {key}: {value.shape}")
        
        if action_parts:
            timesteps = action_parts[0].shape[0]
            total_action_dim = start_idx
            
            normalized_action = np.zeros((timesteps, total_action_dim))
            
            start_idx = 0
            for i, part in enumerate(action_parts):
                part_dim = part.shape[1] if part.ndim > 1 else 1
                if part.ndim > 1:
                    normalized_action[:, start_idx:start_idx+part_dim] = part
                else:
                    normalized_action[:, start_idx] = part
                start_idx += part_dim
            
            normalized_data["action"] = normalized_action
            normalized_data["action_dim"] = total_action_dim
            normalized_data["action_structure"] = action_structure
    
    # å¤åˆ¶å…¶ä»–æ•°æ®ï¼Œä¿æŒåŸå§‹æ ¼å¼
    for key, value in data.items():
        if key not in ["left_arm", "right_arm", "left_gripper", "right_gripper", "format"]:
            normalized_data[key] = value.copy()
    
    print(f"        âœ… æ•°æ®æ ‡å‡†åŒ–å®Œæˆ")
    return normalized_data


def standardize_array_dimensions(arrays_list, target_dim=None):
    """æ ‡å‡†åŒ–æ•°ç»„åˆ—è¡¨çš„ç»´åº¦ï¼Œç¡®ä¿æ‰€æœ‰æ•°ç»„éƒ½æœ‰ç›¸åŒçš„å½¢çŠ¶"""
    if not arrays_list:
        return np.array([])
    
    print(f"    ğŸ”§ æ ‡å‡†åŒ–æ•°ç»„ç»´åº¦...")
    print(f"      åŸå§‹æ•°ç»„æ•°é‡: {len(arrays_list)}")
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ•°ç»„çš„ç»´åº¦
    first_array = arrays_list[0]
    if hasattr(first_array, 'shape'):
        print(f"      ç¬¬ä¸€ä¸ªæ•°ç»„å½¢çŠ¶: {first_array.shape}")
        if first_array.ndim == 1:
            target_dim = first_array.shape[0]
        else:
            target_dim = first_array.flatten().shape[0]
    else:
        print(f"      ç¬¬ä¸€ä¸ªæ•°ç»„ç±»å‹: {type(first_array)}")
        return np.array(arrays_list)
    
    print(f"      ç›®æ ‡ç»´åº¦: {target_dim}")
    
    # æ ‡å‡†åŒ–æ‰€æœ‰æ•°ç»„
    standardized_arrays = []
    for i, arr in enumerate(arrays_list):
        try:
            if hasattr(arr, 'ndim'):
                if arr.ndim == 1:
                    if arr.shape[0] == target_dim:
                        standardized_arrays.append(arr)
                    else:
                        # ç»´åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œpaddingæˆ–truncation
                        if arr.shape[0] < target_dim:
                            # å¡«å……åˆ°ç›®æ ‡ç»´åº¦
                            padded_arr = np.zeros(target_dim, dtype=arr.dtype)
                            padded_arr[:arr.shape[0]] = arr
                            standardized_arrays.append(padded_arr)
                            print(f"        æ•°ç»„{i}: ä»{arr.shape[0]}å¡«å……åˆ°{target_dim}")
                        else:
                            # æˆªæ–­åˆ°ç›®æ ‡ç»´åº¦
                            truncated_arr = arr[:target_dim]
                            standardized_arrays.append(truncated_arr)
                            print(f"        æ•°ç»„{i}: ä»{arr.shape[0]}æˆªæ–­åˆ°{target_dim}")
                else:
                    # å¤šç»´æ•°ç»„ï¼Œå±•å¹³
                    flattened_arr = arr.flatten()
                    if flattened_arr.shape[0] == target_dim:
                        standardized_arrays.append(flattened_arr)
                    else:
                        # å¤„ç†å±•å¹³åçš„ç»´åº¦ä¸åŒ¹é…
                        if flattened_arr.shape[0] < target_dim:
                            padded_arr = np.zeros(target_dim, dtype=flattened_arr.dtype)
                            padded_arr[:flattened_arr.shape[0]] = flattened_arr
                            standardized_arrays.append(padded_arr)
                            print(f"        æ•°ç»„{i}: å±•å¹³åä»{flattened_arr.shape[0]}å¡«å……åˆ°{target_dim}")
                        else:
                            truncated_arr = flattened_arr[:target_dim]
                            standardized_arrays.append(truncated_arr)
                            print(f"        æ•°ç»„{i}: å±•å¹³åä»{flattened_arr.shape[0]}æˆªæ–­åˆ°{target_dim}")
            else:
                print(f"        æ•°ç»„{i}: è·³è¿‡ï¼Œä¸æ˜¯æ•°ç»„ç±»å‹")
                continue
        except Exception as e:
            print(f"        æ•°ç»„{i}: å¤„ç†å¤±è´¥: {e}")
            # åˆ›å»ºé›¶æ•°ç»„ä½œä¸ºfallback
            fallback_arr = np.zeros(target_dim, dtype=np.float32)
            standardized_arrays.append(fallback_arr)
    
    print(f"      æ ‡å‡†åŒ–åæ•°ç»„æ•°é‡: {len(standardized_arrays)}")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    try:
        result = np.array(standardized_arrays)
        print(f"      âœ… æˆåŠŸåˆ›å»ºnumpyæ•°ç»„ï¼Œå½¢çŠ¶: {result.shape}")
        return result
    except Exception as e:
        print(f"      âŒ åˆ›å»ºnumpyæ•°ç»„å¤±è´¥: {e}")
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨objectç±»å‹
        try:
            result = np.array(standardized_arrays, dtype=object)
            print(f"      âš ï¸  ä½¿ç”¨objectç±»å‹åˆ›å»ºæ•°ç»„ï¼Œå½¢çŠ¶: {result.shape}")
            return result
        except Exception as e2:
            print(f"      âŒ å³ä½¿objectç±»å‹ä¹Ÿå¤±è´¥: {e2}")
            return np.array([])


def text2feats(text_inputs: List[str]):
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load("RN50", device=device)
    text_tokens = clip.tokenize(text_inputs).to(device)
    with torch.no_grad():
        text_features = model.encode_text(text_tokens)
        text_feat = text_features.detach().cpu().numpy()
    return text_feat.astype(np.float32)


def validate_embodiment_data_consistency(embodiment_type, episode_list, load_dir):
    """éªŒè¯åŒä¸€æœ¬ä½“å†…æ•°æ®çš„ç»´åº¦ä¸€è‡´æ€§"""
    print(f"      ğŸ” éªŒè¯ {embodiment_type} æ•°æ®ä¸€è‡´æ€§...")
    
    dimensions = []
    episode_lengths = []
    
    for episode_num in episode_list[:3]:  # åªæ£€æŸ¥å‰3ä¸ªepisode
        load_path = os.path.join(load_dir, f"data/episode{episode_num}.hdf5")
        
        try:
            with h5py.File(load_path, "r") as root:
                if "/joint_action" in root:
                    joint_action = root["/joint_action"]
                    
                    if "left_arm" in joint_action and "right_arm" in joint_action:
                        left_arm_shape = joint_action["left_arm"].shape
                        right_arm_shape = joint_action["right_arm"].shape
                        
                        dimensions.append({
                            "episode": episode_num,
                            "left_arm": left_arm_shape,
                            "right_arm": right_arm_shape,
                            "total": left_arm_shape[1] + right_arm_shape[1] if len(left_arm_shape) > 1 and len(right_arm_shape) > 1 else 0
                        })
                        
                        episode_lengths.append(left_arm_shape[0])
                        
        except Exception as e:
            print(f"        è­¦å‘Š: æ— æ³•éªŒè¯episode {episode_num}: {e}")
    
    if dimensions:
        print(f"        ç»´åº¦æ£€æŸ¥ç»“æœ:")
        for dim_info in dimensions:
            print(f"          Episode {dim_info['episode']}: å·¦è‡‚{dim_info['left_arm']}, å³è‡‚{dim_info['right_arm']}")
        
        # æ£€æŸ¥ç»´åº¦ä¸€è‡´æ€§
        left_arm_dims = [d["left_arm"][1] if len(d["left_arm"]) > 1 else 1 for d in dimensions]
        right_arm_dims = [d["right_arm"][1] if len(d["right_arm"]) > 1 else 1 for d in dimensions]
        
        if len(set(left_arm_dims)) > 1:
            print(f"        âš ï¸  è­¦å‘Š: å·¦è‡‚ç»´åº¦ä¸ä¸€è‡´: {left_arm_dims}")
        else:
            print(f"        âœ… å·¦è‡‚ç»´åº¦ä¸€è‡´: {left_arm_dims[0]}")
            
        if len(set(right_arm_dims)) > 1:
            print(f"        âš ï¸  è­¦å‘Š: å³è‡‚ç»´åº¦ä¸ä¸€è‡´: {right_arm_dims}")
        else:
            print(f"        âœ… å³è‡‚ç»´åº¦ä¸€è‡´: {right_arm_dims[0]}")
        
        # æ£€æŸ¥episodeé•¿åº¦ä¸€è‡´æ€§
        if len(set(episode_lengths)) > 1:
            print(f"        âš ï¸  è­¦å‘Š: Episodeé•¿åº¦ä¸ä¸€è‡´: {episode_lengths}")
        else:
            print(f"        âœ… Episodeé•¿åº¦ä¸€è‡´: {episode_lengths[0]}")
    
    return dimensions


def print_data_processing_summary(embodiment_type, episode_list, load_dir):
    """æ‰“å°æ•°æ®å¤„ç†æ‘˜è¦å’Œæ³¨æ„äº‹é¡¹"""
    print(f"\n      ğŸ“Š {embodiment_type} æ•°æ®å¤„ç†æ‘˜è¦")
    print(f"        ========================================")
    print(f"        å¤„ç†çš„episodeæ•°é‡: {len(episode_list)}")
    print(f"        EpisodeèŒƒå›´: {episode_list[0]} - {episode_list[-1]}")
    
    # éªŒè¯æ•°æ®ä¸€è‡´æ€§
    dimensions = validate_embodiment_data_consistency(embodiment_type, episode_list, load_dir)
    
    print(f"\n        âš ï¸  é‡è¦æ³¨æ„äº‹é¡¹:")
    print(f"        1. æ•°æ®ä¿æŒåŸå§‹ç»´åº¦å’Œç‰©ç†æ„ä¹‰")
    print(f"        2. ä¸è¿›è¡Œå¼ºåˆ¶ç»´åº¦ç»Ÿä¸€æˆ–æ•°æ®æˆªæ–­")
    print(f"        3. æ¯ä¸ªæœ¬ä½“ç‹¬ç«‹å¤„ç†ï¼Œé¿å…æ··æ·†")
    print(f"        4. å»ºè®®åœ¨è®­ç»ƒæ—¶åˆ†åˆ«ä½¿ç”¨ä¸åŒæœ¬ä½“çš„æ•°æ®")
    
    if dimensions:
        print(f"\n        ğŸ“‹ å»ºè®®çš„è®­ç»ƒç­–ç•¥:")
        print(f"        - ä¸ºæ¯ä¸ªæœ¬ä½“åˆ›å»ºå•ç‹¬çš„æ¨¡å‹")
        print(f"        - æˆ–è€…ä½¿ç”¨åŠ¨æ€è¾“å…¥å±‚å¤„ç†ä¸åŒç»´åº¦")
        print(f"        - é¿å…æ··åˆè®­ç»ƒä¸åŒæœ¬ä½“çš„æ•°æ®")
    
    print(f"        ========================================")


def text2feats(text_inputs: List[str]):
    # Load model
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load("RN50", device=device)
    text_tokens = clip.tokenize(text_inputs).to(device)
    with torch.no_grad():
        text_features = model.encode_text(text_tokens)
        text_feat = text_features.detach().cpu().numpy()
    return text_feat.astype(np.float32)


def main():
    parser = argparse.ArgumentParser(description="Process some episodes.")
    parser.add_argument(
        "task_name",
        type=str,
        help="The name of the task (e.g., beat_block_hammer)",
    )
    parser.add_argument("task_config", type=str)
    parser.add_argument(
        "expert_data_num",
        type=int,
        help="Number of episodes to process (e.g., 50)",
    )
    args = parser.parse_args()

    task_name = args.task_name
    num = args.expert_data_num
    task_config = args.task_config

    # Convert task name to text features
    task_text = task_name.replace("_", " ")
    text_feat = text2feats([task_text])
    print(f"Task: {task_name}, Text: '{task_text}', Text feature shape: {text_feat.shape}")

    load_dir = "../../data/" + str(task_name) + "/" + str(task_config)

    # é‡æ–°è®¾è®¡ï¼šä¸ºæ¯ä¸ªæœ¬ä½“åˆ†åˆ«å¤„ç†æ•°æ®
    print("=== å¤šæœ¬ä½“æ•°æ®å¤„ç† ===")
    print("ä¸ºé¿å…æ•°æ®ä¸¢å¤±å’Œç‰©ç†æ„ä¹‰æ··æ·†ï¼Œå°†ä¸ºæ¯ä¸ªæœ¬ä½“åˆ›å»ºå•ç‹¬çš„æ•°æ®æ–‡ä»¶")
    
    # å®šä¹‰æœ¬ä½“åˆ†ç»„ï¼ˆåŸºäºmerge_data.shä¸­çš„å®é™…é¡ºåºï¼‰
    embodiment_groups = {
        "ur5-wsg": {"start": 0, "end": 50, "episodes": []},
        "franka-panda": {"start": 50, "end": 100, "episodes": []},
        "ARX-X5": {"start": 100, "end": 150, "episodes": []},
        "aloha-agilex": {"start": 150, "end": 200, "episodes": []}
    }
    
    # ç¬¬ä¸€éï¼šæ”¶é›†æ¯ä¸ªæœ¬ä½“çš„episodeä¿¡æ¯
    print("\n=== ç¬¬ä¸€éï¼šæ”¶é›†episodeä¿¡æ¯ ===")
    for current_ep in range(num):
        load_path = os.path.join(load_dir, f"data/episode{current_ep}.hdf5")
        
        # ç¡®å®šepisodeå±äºå“ªä¸ªæœ¬ä½“
        embodiment_type = None
        for emb_type, group in embodiment_groups.items():
            if group["start"] <= current_ep < group["end"]:
                embodiment_type = emb_type
                break
        
        if embodiment_type:
            embodiment_groups[embodiment_type]["episodes"].append(current_ep)
            # ç²¾ç®€è¾“å‡ºï¼šåªæ˜¾ç¤ºæ¯10ä¸ªepisodeçš„è¿›åº¦
            if current_ep % 10 == 0 or current_ep == num - 1:
                print(f"Episode {current_ep} â†’ {embodiment_type}")
    
    # æ˜¾ç¤ºåˆ†ç»„ç»“æœ
    print("\nğŸ“Š Episodeåˆ†ç»„ç»“æœ:")
    for emb_type, group in embodiment_groups.items():
        print(f"  {emb_type}: {len(group['episodes'])} episodes")
    
    # ç¬¬äºŒéï¼šä¸ºæ¯ä¸ªæœ¬ä½“åˆ†åˆ«å¤„ç†æ•°æ®
    print("\n=== ç¬¬äºŒéï¼šåˆ†åˆ«å¤„ç†æ¯ä¸ªæœ¬ä½“ ===")
    for embodiment_type, group in embodiment_groups.items():
        if not group["episodes"]:
            print(f"âš ï¸  {embodiment_type}: æ²¡æœ‰episodeæ•°æ®")
            continue
            
        print(f"\n--- å¤„ç† {embodiment_type} ({len(group['episodes'])} episodes) ---")
        
        # ç²¾ç®€æ‘˜è¦ä¿¡æ¯
        print(f"  ğŸ“Š {embodiment_type}: {len(group['episodes'])} episodes, èŒƒå›´: {group['episodes'][0]}-{group['episodes'][-1]}")
        
        # ä¸ºæ¯ä¸ªæœ¬ä½“åˆ›å»ºå•ç‹¬çš„è¾“å‡ºæ–‡ä»¶
        save_dir = f"./data/{task_name}-{task_config}-{embodiment_type}-{len(group['episodes'])}.zarr"

    if os.path.exists(save_dir):
        shutil.rmtree(save_dir)

        # å¤„ç†è¿™ä¸ªæœ¬ä½“çš„æ‰€æœ‰episode
        process_embodiment_episodes(
            embodiment_type, 
            group["episodes"], 
            load_dir, 
            save_dir, 
            task_name
        )
    
    print(f"\n=== å¤„ç†å®Œæˆ ===")
    print("æ¯ä¸ªæœ¬ä½“éƒ½æœ‰ç‹¬ç«‹çš„æ•°æ®æ–‡ä»¶ï¼Œé¿å…äº†æ•°æ®ä¸¢å¤±å’Œç‰©ç†æ„ä¹‰æ··æ·†")


def process_embodiment_episodes(embodiment_type, episode_list, load_dir, save_dir, task_name):
    """å¤„ç†å•ä¸ªæœ¬ä½“çš„æ‰€æœ‰episode"""
    print(f"  å¼€å§‹å¤„ç† {embodiment_type} çš„ {len(episode_list)} ä¸ªepisode...")
    
    # åˆ›å»ºzarræ–‡ä»¶
    zarr_root = zarr.group(save_dir)
    zarr_data = zarr_root.create_group("data")
    zarr_meta = zarr_root.create_group("meta")

    head_camera_arrays, front_camera_arrays, left_camera_arrays, right_camera_arrays = (
        [],
        [],
        [],
        [],
    )
    episode_ends_arrays, action_arrays, state_arrays, joint_action_arrays, action_mask_arrays = (
        [],
        [],
        [],
        [],
        [],
    )

    while current_ep < num:
        print(f"processing episode: {current_ep + 1} / {num}", end="\r")

        load_path = os.path.join(load_dir, f"data/episode{current_ep}.hdf5")
        (
            left_gripper_all,
            left_arm_all,
            right_gripper_all,
            right_arm_all,
            vector_all,
            image_dict_all,
        ) = load_hdf5(load_path)

        for j in range(0, left_gripper_all.shape[0]):

            # å¤„ç†å›¾åƒå’ŒçŠ¶æ€æ•°æ®
            if "image_dict" in normalized_data and "head_camera" in normalized_data["image_dict"]:
                image_dict_all = normalized_data["image_dict"]
                episode_length = image_dict_all["head_camera"].shape[0]
                
                # ç²¾ç®€è¾“å‡ºï¼šåªæ˜¾ç¤ºå¼‚å¸¸é•¿åº¦
                if episode_length < 10 or episode_length > 1000:
                    print(f"      âš ï¸  Episodeé•¿åº¦å¼‚å¸¸: {episode_length}")
                
                for j in range(episode_length):
            head_img_bit = image_dict_all["head_camera"][j]
            joint_state = vector_all[j]

            # Detect action dimension and apply padding
            action_dim = joint_state.shape[-1] if joint_state.ndim > 0 else 1
            if action_dim == 7:  # 6 joints + 1 gripper (single arm)
                # Pad to 16 dimensions, set last 9 dimensions to 0
                padded_action = np.pad(joint_state, (0, 9), mode='constant', constant_values=0)
                action_mask = np.zeros(16, dtype=np.float32)
                action_mask[:7] = 1  # First 7 dimensions are valid (6 joints + 1 gripper)
            elif action_dim == 8:  # 7 joints + 1 gripper (single arm)
                # Pad to 16 dimensions, set last 8 dimensions to 0
                padded_action = np.pad(joint_state, (0, 8), mode='constant', constant_values=0)
                action_mask = np.zeros(16, dtype=np.float32)
                action_mask[:8] = 1  # First 8 dimensions are valid (7 joints + 1 gripper)
            elif action_dim == 14:  # Dual arm: (6 joints + 1 gripper) * 2 = 14
                # Pad to 16 dimensions, set last 2 dimensions to 0
                padded_action = np.pad(joint_state, (0, 2), mode='constant', constant_values=0)
                action_mask = np.ones(16, dtype=np.float32)
                action_mask[-2:] = 0  # Last 2 dimensions are invalid
            elif action_dim == 16:  # Dual arm: (7 joints + 1 gripper) * 2 = 16
                padded_action = joint_state
                action_mask = np.ones(16, dtype=np.float32)
            else:
                raise ValueError(f"Unsupported action dimension: {action_dim}. Expected 7, 8, 14, or 16.")

            if j != left_gripper_all.shape[0] - 1:
                head_img = cv2.imdecode(np.frombuffer(head_img_bit, np.uint8), cv2.IMREAD_COLOR)
                        head_img_tensor = torch.from_numpy(head_img).float().permute(2, 0, 1).unsqueeze(0)
                head_img_resized = torch.nn.functional.interpolate(head_img_tensor, size=(256, 256), mode='bilinear', align_corners=False)
                        head_img = head_img_resized.squeeze(0).permute(1, 2, 0).cpu().numpy().astype(np.uint8)
                        
                head_camera_arrays.append(head_img)
                state_arrays.append(padded_action)  # Use padded action as state
                action_mask_arrays.append(action_mask)
            if j != 0:
                joint_action_arrays.append(padded_action)  # Use padded action

        current_ep += 1
        total_count += left_gripper_all.shape[0] - 1
        episode_ends_arrays.append(total_count)

                # ç²¾ç®€è¾“å‡ºï¼šåªæ˜¾ç¤ºå¼‚å¸¸æƒ…å†µ
                if episode_length - 1 < 5:
                    print(f"      âš ï¸  Episodeæ—¶é—´æ­¥è¿‡å°‘: {episode_length - 1}")
                
        except Exception as e:
            print(f"      âŒ å¤„ç†episode {episode_num} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ä¿å­˜æ•°æ®
    if not state_arrays or not head_camera_arrays:
        print(f"  âŒ {embodiment_type}: æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡ä¿å­˜")
        return
    
    print(f"  ğŸ’¾ ä¿å­˜ {embodiment_type} æ•°æ®...")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆç°åœ¨æ‰€æœ‰æ•°ç»„éƒ½æœ‰ç›¸åŒçš„ç»´åº¦ï¼‰
    episode_ends_arrays = np.array(episode_ends_arrays)
    state_arrays = np.array(state_arrays)
    head_camera_arrays = np.array(head_camera_arrays)
    
    if joint_action_arrays:
    joint_action_arrays = np.array(joint_action_arrays)
    action_mask_arrays = np.array(action_mask_arrays)

    print(f"Processed data shapes:")
    print(f"  State: {state_arrays.shape}")
    print(f"  Action: {joint_action_arrays.shape}")
    print(f"  Action Mask: {action_mask_arrays.shape}")
    print(f"  Head Camera: {head_camera_arrays.shape}")
    print(f"  Episodes: {len(episode_ends_arrays)}")

    head_camera_arrays = np.moveaxis(head_camera_arrays, -1, 1)  # NHWC -> NCHW

    # ä¿å­˜åˆ°zarr
    compressor = zarr.Blosc(cname="zstd", clevel=3, shuffle=1)
    # action_chunk_size = (100, action_arrays.shape[1])
    state_chunk_size = (100, 16)  # Fixed to 16 dimensions
    joint_chunk_size = (100, 16)  # Fixed to 16 dimensions
    action_mask_chunk_size = (100, 16)  # Fixed to 16 dimensions
    head_camera_chunk_size = (100, *head_camera_arrays.shape[1:])
    
    # ä¿å­˜æ•°æ®
    safe_create_zarr_dataset(
        zarr_data, "head_cam",
        data=head_camera_arrays,
        chunks=head_camera_chunk_size,
        overwrite=True,
        compressor=compressor,
    )
    
    safe_create_zarr_dataset(
        zarr_data, "state",
        data=state_arrays,
        chunks=state_chunk_size,
        overwrite=True,
        compressor=compressor,
    )
    
    safe_create_zarr_dataset(
        zarr_data, "action",
        data=joint_action_arrays,
        chunks=joint_chunk_size,
        overwrite=True,
        compressor=compressor,
    )
    zarr_data.create_dataset(
        "action_mask",
        data=action_mask_arrays,
        chunks=action_mask_chunk_size,
        dtype="float32",
        overwrite=True,
        compressor=compressor,
    )
    zarr_data.create_dataset(
        "text_feat",
        data=text_feat,
        dtype="float32",
        overwrite=True,
        compressor=compressor,
    )
    zarr_meta.create_dataset(
        "episode_ends",
        data=episode_ends_arrays,
        overwrite=True,
        compressor=compressor,
    )
    
    # ä¿å­˜embodimentä¿¡æ¯
    safe_create_zarr_dataset(
        zarr_meta, "embodiment_type",
        data=embodiment_type,
        overwrite=True,
    )
    
    print(f"  âœ… {embodiment_type} æ•°æ®ä¿å­˜å®Œæˆ: {save_dir}")
    print(f"     æ€»æ—¶é—´æ­¥: {total_count}, çŠ¶æ€æ•°ç»„: {state_arrays.shape}, åŠ¨ä½œæ•°ç»„: {joint_action_arrays.shape}")


def safe_create_zarr_dataset(zarr_group, name, data, **kwargs):
    """å®‰å…¨åœ°åˆ›å»ºZarræ•°æ®é›†ï¼Œè‡ªåŠ¨å¤„ç†æ•°æ®ç±»å‹"""
    try:
        # åˆ›å»ºkwargsçš„å‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹å‚æ•°
        safe_kwargs = kwargs.copy()
        
        # è‡ªåŠ¨æ¨æ–­åˆé€‚çš„æ•°æ®ç±»å‹
        if isinstance(data, str):
            # å­—ç¬¦ä¸²æ•°æ®
            max_length = len(data) + 10  # ç•™ä¸€äº›ä½™é‡
            safe_kwargs['dtype'] = f"U{max_length}"
            # å­—ç¬¦ä¸²æ•°æ®ä¸ä½¿ç”¨å‹ç¼©å™¨
            safe_kwargs.pop('compressor', None)
        elif isinstance(data, (list, tuple)) and len(data) > 0:
            # åˆ—è¡¨æˆ–å…ƒç»„æ•°æ®
            if isinstance(data[0], str):
                # å­—ç¬¦ä¸²åˆ—è¡¨
                max_length = max(len(s) for s in data) + 10
                safe_kwargs['dtype'] = f"U{max_length}"
                safe_kwargs.pop('compressor', None)
            else:
                # æ•°å€¼åˆ—è¡¨ï¼Œè®©numpyè‡ªåŠ¨æ¨æ–­
                # ä¸è®¾ç½®dtypeï¼Œè®©numpyè‡ªåŠ¨æ¨æ–­
                pass
        elif hasattr(data, 'dtype'):
            # numpyæ•°ç»„ï¼Œæ ¹æ®æ•°æ®ç±»å‹è®¾ç½®åˆé€‚çš„dtype
            if np.issubdtype(data.dtype, np.floating):
                if data.dtype == np.float64:
                    safe_kwargs['dtype'] = 'float64'
                else:
                    safe_kwargs['dtype'] = 'float32'
            elif np.issubdtype(data.dtype, np.integer):
                if data.dtype == np.int64:
                    safe_kwargs['dtype'] = 'int64'
                else:
                    safe_kwargs['dtype'] = 'int32'
            # å…¶ä»–ç±»å‹è®©numpyè‡ªåŠ¨æ¨æ–­
        else:
            # å…¶ä»–æ•°æ®ç±»å‹ï¼Œè®©numpyè‡ªåŠ¨æ¨æ–­
            # ä¸è®¾ç½®dtypeï¼Œè®©numpyè‡ªåŠ¨æ¨æ–­
            pass
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = zarr_group.create_dataset(
            name,
            data=data,
            **safe_kwargs
        )
        
        return dataset
        
    except Exception as e:
        print(f"      âš ï¸  åˆ›å»ºæ•°æ®é›† {name} å¤±è´¥: {e}")
        # å°è¯•ä½¿ç”¨objectç±»å‹ä½œä¸ºfallback
        try:
            fallback_kwargs = kwargs.copy()
            fallback_kwargs.pop('compressor', None)  # objectç±»å‹ä¸ä½¿ç”¨å‹ç¼©å™¨
            fallback_kwargs['dtype'] = object
            
            dataset = zarr_group.create_dataset(
                name,
                data=data,
                **fallback_kwargs
            )
            print(f"      âœ… ä½¿ç”¨objectç±»å‹æˆåŠŸåˆ›å»º {name}")
            return dataset
        except Exception as e2:
            print(f"      âŒ å³ä½¿objectç±»å‹ä¹Ÿå¤±è´¥: {e2}")
            raise e2


if __name__ == "__main__":
    main()
