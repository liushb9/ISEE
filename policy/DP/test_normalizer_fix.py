#!/usr/bin/env python3
"""
æµ‹è¯•normalizerä¿®å¤
éªŒè¯agent_poså½’ä¸€åŒ–å‚æ•°æ˜¯å¦æ­£ç¡®è®¾ç½®
"""

import sys
import os

# è®¾ç½®æ­£ç¡®çš„Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

def test_normalizer_initialization():
    """æµ‹è¯•normalizeråˆå§‹åŒ–"""
    print("=== æµ‹è¯•normalizeråˆå§‹åŒ– ===")
    
    try:
        # æµ‹è¯•å¯¼å…¥
        from diffusion_policy.dataset.robot_image_dataset import RobotImageDataset
        print("âœ… RobotImageDatasetå¯¼å…¥æˆåŠŸ")
        
        # æ£€æŸ¥æ•°æ®é›†è·¯å¾„
        zarr_path = "data/six-tasks.zarr"
        if not os.path.exists(zarr_path):
            print(f"âŒ æ•°æ®é›†ä¸å­˜åœ¨: {zarr_path}")
            print("è¯·å…ˆè¿è¡Œæ•°æ®å¤„ç†è„šæœ¬åˆ›å»ºæ•°æ®é›†")
            return False
        
        # åˆ›å»ºæ•°æ®é›†å®ä¾‹
        dataset = RobotImageDataset(
            zarr_path=zarr_path,
            horizon=1,
            pad_before=0,
            pad_after=0,
            seed=42,
            val_ratio=0.0,
            batch_size=128,
            max_train_episodes=None,
        )
        print("âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸ")
        
        # æ£€æŸ¥æ•°æ®ç»´åº¦
        action_dim = dataset.replay_buffer["action"].shape[1]
        state_dim = dataset.replay_buffer["state"].shape[1]
        print(f"   Actionç»´åº¦: {action_dim}")
        print(f"   Stateç»´åº¦: {state_dim}")
        
        # è·å–normalizer
        print("\nğŸ”§ è·å–normalizer...")
        normalizer = dataset.get_normalizer()
        print("âœ… normalizerè·å–æˆåŠŸ")
        
        # æ£€æŸ¥normalizerå‚æ•°
        print("\nğŸ“Š æ£€æŸ¥normalizerå‚æ•°...")
        
        # æ£€æŸ¥actionå‚æ•°
        if "action" in normalizer.params_dict:
            print("âœ… actionå‚æ•°å­˜åœ¨")
            action_params = normalizer.params_dict["action"]
            if "scale" in action_params and "offset" in action_params:
                print(f"   Action scale shape: {action_params['scale'].shape}")
                print(f"   Action offset shape: {action_params['offset'].shape}")
            else:
                print("âŒ actionå‚æ•°ä¸å®Œæ•´")
                return False
        else:
            print("âŒ actionå‚æ•°ä¸å­˜åœ¨")
            return False
        
        # æ£€æŸ¥agent_poså‚æ•°
        if "agent_pos" in normalizer.params_dict:
            print("âœ… agent_poså‚æ•°å­˜åœ¨")
            agent_pos_params = normalizer.params_dict["agent_pos"]
            if "scale" in agent_pos_params and "offset" in agent_pos_params:
                print(f"   Agent_pos scale shape: {agent_pos_params['scale'].shape}")
                print(f"   Agent_pos offset shape: {agent_pos_params['offset'].shape}")
            else:
                print("âŒ agent_poså‚æ•°ä¸å®Œæ•´")
                return False
        else:
            print("âŒ agent_poså‚æ•°ä¸å­˜åœ¨")
            return False
        
        # æ£€æŸ¥å›¾åƒå‚æ•°
        image_keys = ["head_cam", "front_cam", "left_cam", "right_cam"]
        for key in image_keys:
            if key in normalizer.params_dict:
                print(f"âœ… {key}å‚æ•°å­˜åœ¨")
            else:
                print(f"âš ï¸  {key}å‚æ•°ä¸å­˜åœ¨ï¼ˆå¯èƒ½æ­£å¸¸ï¼‰")
        
        print("\nâœ… normalizeråˆå§‹åŒ–æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ normalizeråˆå§‹åŒ–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_normalizer_usage():
    """æµ‹è¯•normalizerä½¿ç”¨"""
    print("\n=== æµ‹è¯•normalizerä½¿ç”¨ ===")
    
    try:
        # æµ‹è¯•å¯¼å…¥
        from diffusion_policy.dataset.robot_image_dataset import RobotImageDataset
        
        # åˆ›å»ºæ•°æ®é›†å®ä¾‹
        zarr_path = "data/six-tasks.zarr"
        if not os.path.exists(zarr_path):
            print(f"âŒ æ•°æ®é›†ä¸å­˜åœ¨: {zarr_path}")
            return False
        
        dataset = RobotImageDataset(
            zarr_path=zarr_path,
            horizon=1,
            pad_before=0,
            pad_after=0,
            seed=42,
            val_ratio=0.0,
            batch_size=128,
            max_train_episodes=None,
        )
        
        # è·å–normalizer
        normalizer = dataset.get_normalizer()
        
        # æµ‹è¯•å½’ä¸€åŒ–
        print("ğŸ”§ æµ‹è¯•å½’ä¸€åŒ–...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        import torch
        test_batch = {
            "obs": {
                "head_cam": torch.randn(2, 3, 256, 256),
                "agent_pos": torch.randn(2, 10),
                "text_feat": torch.randn(2, 512)
            },
            "action": torch.randn(2, 10)
        }
        
        # æµ‹è¯•å½’ä¸€åŒ–
        try:
            normalized_batch = normalizer.normalize(test_batch)
            print("âœ… å½’ä¸€åŒ–æˆåŠŸ")
            print(f"   å½’ä¸€åŒ–åaction shape: {normalized_batch['action'].shape}")
            print(f"   å½’ä¸€åŒ–åagent_pos shape: {normalized_batch['obs']['agent_pos'].shape}")
        except Exception as e:
            print(f"âŒ å½’ä¸€åŒ–å¤±è´¥: {e}")
            return False
        
        print("âœ… normalizerä½¿ç”¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ normalizerä½¿ç”¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("=== Normalizerä¿®å¤æµ‹è¯• ===")
    
    # æµ‹è¯•1: normalizeråˆå§‹åŒ–
    init_ok = test_normalizer_initialization()
    
    # æµ‹è¯•2: normalizerä½¿ç”¨
    usage_ok = test_normalizer_usage()
    
    # æ€»ç»“
    print(f"\n=== æµ‹è¯•æ€»ç»“ ===")
    print(f"Normalizeråˆå§‹åŒ–: {'âœ…' if init_ok else 'âŒ'}")
    print(f"Normalizerä½¿ç”¨: {'âœ…' if usage_ok else 'âŒ'}")
    
    if init_ok and usage_ok:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Normalizerä¿®å¤æˆåŠŸ")
        print("\nç°åœ¨å¯ä»¥é‡æ–°è¿è¡Œè®­ç»ƒ:")
        print("bash train_multi_gpu.sh six_tasks demo_clean 1200 0 1 '0'")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")

if __name__ == "__main__":
    main()
