#!/usr/bin/env python3
"""
æµ‹è¯•æµå¼å½’ä¸€åŒ–å™¨
éªŒè¯ä¿®æ”¹åçš„normalizer.fit()æ˜¯å¦çœŸæ­£é¿å…äº†å†…å­˜çˆ†ç‚¸
"""

import numpy as np
import sys
import os
import psutil
import gc
import time

# æ·»åŠ è·¯å¾„
sys.path.append('/home/shengbang/RoboTwin/policy/DP')

def create_large_mock_dataset():
    """åˆ›å»ºå¤§å‹æ¨¡æ‹Ÿæ•°æ®é›†"""
    print("ğŸ“Š åˆ›å»ºå¤§å‹æ¨¡æ‹Ÿæ•°æ®é›†...")
    
    # æ¨¡æ‹Ÿ100ä¸‡ä¸ªæ—¶é—´æ­¥ï¼Œ10ç»´action
    n_timesteps = 1_000_000
    action_dim = 10
    
    # åˆ›å»ºéšæœºæ•°æ®
    action_data = np.random.randn(n_timesteps, action_dim).astype(np.float32)
    
    print(f"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ")
    print(f"   - æ—¶é—´æ­¥æ•°: {n_timesteps:,}")
    print(f"   - Actionç»´åº¦: {action_dim}")
    print(f"   - æ•°æ®å¤§å°: {action_data.nbytes / 1024 / 1024:.2f} MB")
    
    return action_data

def test_traditional_normalizer(data):
    """æµ‹è¯•ä¼ ç»Ÿå½’ä¸€åŒ–æ–¹æ³•"""
    print("\nğŸ”„ æµ‹è¯•ä¼ ç»Ÿå½’ä¸€åŒ–æ–¹æ³•...")
    
    from diffusion_policy.model.common.normalizer import LinearNormalizer
    
    process = psutil.Process()
    mem_before = process.memory_info().rss / 1024 / 1024  # MB
    
    print(f"   å†…å­˜ä½¿ç”¨å‰: {mem_before:.2f} MB")
    
    # åˆ›å»ºå½’ä¸€åŒ–å™¨
    normalizer = LinearNormalizer()
    
    # æµ‹è¯•ä¼ ç»Ÿæ–¹æ³•ï¼ˆuse_streaming=Falseï¼‰
    start_time = time.time()
    try:
        normalizer.fit(
            data={"action": data},
            last_n_dims=1,
            mode="limits",
            use_streaming=False  # å¼ºåˆ¶ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
        )
        end_time = time.time()
        
        mem_after = process.memory_info().rss / 1024 / 1024
        print(f"   å†…å­˜ä½¿ç”¨å: {mem_after:.2f} MB (+{mem_after - mem_before:.2f} MB)")
        print(f"   è®¡ç®—æ—¶é—´: {end_time - start_time:.4f} ç§’")
        
        # æ£€æŸ¥å½’ä¸€åŒ–å™¨å‚æ•°
        if "action" in normalizer.params_dict:
            params = normalizer.params_dict["action"]
            print(f"   âœ… å½’ä¸€åŒ–å™¨åˆ›å»ºæˆåŠŸ")
            print(f"      Scale shape: {params['scale'].shape}")
            print(f"      Offset shape: {params['offset'].shape}")
        else:
            print(f"   âŒ å½’ä¸€åŒ–å™¨åˆ›å»ºå¤±è´¥")
            
    except Exception as e:
        print(f"   âŒ ä¼ ç»Ÿæ–¹æ³•å¤±è´¥: {e}")
        return None
    
    return normalizer

def test_streaming_normalizer(data):
    """æµ‹è¯•æµå¼å½’ä¸€åŒ–æ–¹æ³•"""
    print("\nğŸš€ æµ‹è¯•æµå¼å½’ä¸€åŒ–æ–¹æ³•...")
    
    from diffusion_policy.model.common.normalizer import LinearNormalizer
    
    process = psutil.Process()
    mem_before = process.memory_info().rss / 1024 / 1024  # MB
    
    print(f"   å†…å­˜ä½¿ç”¨å‰: {mem_before:.2f} MB")
    
    # åˆ›å»ºå½’ä¸€åŒ–å™¨
    normalizer = LinearNormalizer()
    
    # æµ‹è¯•æµå¼æ–¹æ³•ï¼ˆuse_streaming=Trueï¼Œé»˜è®¤å€¼ï¼‰
    start_time = time.time()
    try:
        normalizer.fit(
            data={"action": data},
            last_n_dims=1,
            mode="limits",
            use_streaming=True  # ä½¿ç”¨æµå¼æ–¹æ³•
        )
        end_time = time.time()
        
        mem_after = process.memory_info().rss / 1024 / 1024
        print(f"   å†…å­˜ä½¿ç”¨å: {mem_after:.2f} MB (+{mem_after - mem_before:.2f} MB)")
        print(f"   è®¡ç®—æ—¶é—´: {end_time - start_time:.4f} ç§’")
        
        # æ£€æŸ¥å½’ä¸€åŒ–å™¨å‚æ•°
        if "action" in normalizer.params_dict:
            params = normalizer.params_dict["action"]
            print(f"   âœ… å½’ä¸€åŒ–å™¨åˆ›å»ºæˆåŠŸ")
            print(f"      Scale shape: {params['scale'].shape}")
            print(f"      Offset shape: {params['offset'].shape}")
        else:
            print(f"   âŒ å½’ä¸€åŒ–å™¨åˆ›å»ºå¤±è´¥")
            
    except Exception as e:
        print(f"   âŒ æµå¼æ–¹æ³•å¤±è´¥: {e}")
        return None
    
    return normalizer

def compare_normalizers(traditional_norm, streaming_norm):
    """æ¯”è¾ƒä¸¤ç§å½’ä¸€åŒ–å™¨çš„ç»“æœ"""
    if traditional_norm is None or streaming_norm is None:
        print("âŒ æ— æ³•æ¯”è¾ƒï¼Œå½’ä¸€åŒ–å™¨åˆ›å»ºå¤±è´¥")
        return
    
    print("\nğŸ“Š å½’ä¸€åŒ–å™¨ç»“æœå¯¹æ¯”:")
    print("=" * 50)
    
    try:
        # æ¯”è¾ƒå‚æ•°
        trad_params = traditional_norm.params_dict["action"]
        stream_params = streaming_norm.params_dict["action"]
        
        # æ¯”è¾ƒscale
        scale_diff = torch.max(torch.abs(trad_params["scale"] - stream_params["scale"]))
        print(f"Scaleå·®å¼‚: {scale_diff:.2e}")
        
        # æ¯”è¾ƒoffset
        offset_diff = torch.max(torch.abs(trad_params["offset"] - stream_params["offset"]))
        print(f"Offsetå·®å¼‚: {offset_diff:.2e}")
        
        # æ¯”è¾ƒç»Ÿè®¡ä¿¡æ¯
        trad_stats = trad_params["input_stats"]
        stream_stats = stream_params["input_stats"]
        
        min_diff = torch.max(torch.abs(trad_stats["min"] - stream_stats["min"]))
        max_diff = torch.max(torch.abs(trad_stats["max"] - stream_stats["max"]))
        mean_diff = torch.max(torch.abs(trad_stats["mean"] - stream_stats["mean"]))
        std_diff = torch.max(torch.abs(trad_stats["std"] - stream_stats["std"]))
        
        print(f"Minå·®å¼‚: {min_diff:.2e}")
        print(f"Maxå·®å¼‚: {max_diff:.2e}")
        print(f"Meanå·®å¼‚: {mean_diff:.2e}")
        print(f"Stdå·®å¼‚: {std_diff:.2e}")
        
        if max_diff < 1e-10:
            print("âœ… ç»“æœå®Œå…¨ä¸€è‡´ï¼")
        else:
            print("âš ï¸  ç»“æœå­˜åœ¨å¾®å°å·®å¼‚")
            
    except Exception as e:
        print(f"âŒ æ¯”è¾ƒå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("=== æµå¼å½’ä¸€åŒ–å™¨æµ‹è¯• ===")
    
    # åˆ›å»ºå¤§å‹æ•°æ®é›†
    action_data = create_large_mock_dataset()
    
    # æµ‹è¯•ä¼ ç»Ÿæ–¹æ³•
    traditional_norm = test_traditional_normalizer(action_data)
    
    # æ¸…ç†å†…å­˜
    del traditional_norm
    gc.collect()
    
    # æµ‹è¯•æµå¼æ–¹æ³•
    streaming_norm = test_streaming_normalizer(action_data)
    
    # æ¯”è¾ƒç»“æœ
    compare_normalizers(traditional_norm, streaming_norm)
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    main()
