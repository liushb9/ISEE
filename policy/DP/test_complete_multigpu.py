#!/usr/bin/env python3
"""
å®Œæ•´çš„å¤šå¡è®­ç»ƒç¯å¢ƒæµ‹è¯•è„šæœ¬
éªŒè¯æ‰€æœ‰ç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import os
import torch

# è®¾ç½®æ­£ç¡®çš„Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

def test_environment():
    """æµ‹è¯•ç¯å¢ƒé…ç½®"""
    print("=== ç¯å¢ƒé…ç½®æµ‹è¯• ===")
    
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    
    if torch.cuda.is_available():
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
    
    return True

def test_lightning():
    """æµ‹è¯•LightningåŠŸèƒ½"""
    print("\n=== LightningåŠŸèƒ½æµ‹è¯• ===")
    
    try:
        import lightning
        print(f"âœ… Lightningç‰ˆæœ¬: {lightning.__version__}")
        
        from lightning.fabric import Fabric
        print("âœ… Fabricå¯ç”¨")
        
        from lightning.pytorch.strategies import DDPStrategy
        print("âœ… DDPç­–ç•¥å¯ç”¨")
        
        return True
        
    except Exception as e:
        print(f"âŒ Lightningæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_fabric_creation():
    """æµ‹è¯•Fabricå®ä¾‹åˆ›å»º"""
    print("\n=== Fabricå®ä¾‹æµ‹è¯• ===")
    
    try:
        from lightning.fabric import Fabric
        
        # åˆ›å»ºFabricå®ä¾‹
        fabric = Fabric(
            accelerator="cuda" if torch.cuda.is_available() else "cpu",
            devices=min(2, torch.cuda.device_count()) if torch.cuda.is_available() else 1,
            strategy="ddp",
            precision="32-true",
        )
        
        print("âœ… Fabricå®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å…³é”®å±æ€§
        print(f"   åŠ é€Ÿå™¨: {fabric.accelerator}")
        print(f"   ç­–ç•¥: {fabric.strategy}")
        
        # å®‰å…¨åœ°è·å–è®¾å¤‡æ•°é‡
        try:
            if hasattr(fabric, 'num_devices'):
                device_count = fabric.num_devices
            elif hasattr(fabric, 'devices'):
                device_count = fabric.devices
            elif hasattr(fabric, 'num_gpus'):
                device_count = fabric.num_gpus
            else:
                device_count = min(2, torch.cuda.device_count()) if torch.cuda.is_available() else 1
            print(f"   è®¾å¤‡æ•°é‡: {device_count}")
        except Exception as e:
            print(f"   âš ï¸  æ— æ³•è·å–è®¾å¤‡æ•°é‡: {e}")
        
        return fabric
        
    except Exception as e:
        print(f"âŒ Fabricåˆ›å»ºå¤±è´¥: {e}")
        return None

def test_module_imports():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    print("\n=== æ¨¡å—å¯¼å…¥æµ‹è¯• ===")
    
    modules_to_test = [
        "diffusion_policy.workspace.robotworkspace.RobotWorkspace",
        "diffusion_policy.dataset.robot_image_dataset.RobotImageDataset",
        "diffusion_policy.policy.diffusion_unet_image_policy.DiffusionUnetImagePolicy",
        "diffusion_policy.model.common.normalizer.LinearNormalizer",
    ]
    
    all_imports_ok = True
    
    for module_name in modules_to_test:
        try:
            module = __import__(module_name, fromlist=[''])
            print(f"âœ… {module_name}")
        except Exception as e:
            print(f"âŒ {module_name}: {e}")
            all_imports_ok = False
    
    return all_imports_ok

def test_workspace_creation():
    """æµ‹è¯•Workspaceåˆ›å»º"""
    print("\n=== Workspaceåˆ›å»ºæµ‹è¯• ===")
    
    try:
        from diffusion_policy.workspace.robotworkspace import RobotWorkspace
        from omegaconf import OmegaConf
        
        # åˆ›å»ºæœ€å°é…ç½®
        cfg = OmegaConf.create({
            "task": {
                "name": "test",
                "dataset": {
                    "_target_": "diffusion_policy.dataset.robot_image_dataset.RobotImageDataset",
                    "zarr_path": "data/test.zarr",
                    "horizon": 1,
                    "pad_before": 0,
                    "pad_after": 0,
                    "seed": 42,
                    "val_ratio": 0.0,
                    "batch_size": 1,
                    "max_train_episodes": None
                },
                "env_runner": None
            },
            "policy": {
                "_target_": "diffusion_policy.policy.diffusion_unet_image_policy.DiffusionUnetImagePolicy",
                "obs_encoder": {
                    "_target_": "diffusion_policy.model.common.mlp.MLP",
                    "input_dim": 10,
                    "output_dim": 64,
                    "hidden_dims": [64, 64]
                },
                "action_dim": 10,
                "n_obs_steps": 2,
                "n_action_steps": 2,
                "n_cond_steps": 2,
                "horizon": 4,
                "obs_as_global_cond": True,
                "diffusion_step_embed_dim": 64,
                "down_dims": [64, 128, 256],
                "up_dims": [256, 128, 64],
                "n_groups": 8,
                "mid_blocks_per_group": 2,
                "use_fp16": False
            },
            "optimizer": {
                "_target_": "torch.optim.AdamW",
                "lr": 1e-4,
                "weight_decay": 1e-6
            },
            "dataloader": {
                "batch_size": 1,
                "num_workers": 0,
                "shuffle": True,
                "pin_memory": False,
                "persistent_workers": False
            },
            "val_dataloader": {
                "batch_size": 1,
                "num_workers": 0,
                "shuffle": False,
                "pin_memory": False,
                "persistent_workers": False
            },
            "training": {
                "seed": 42,
                "device": "cuda:0",
                "num_epochs": 1,
                "max_train_steps": None,
                "max_val_steps": None,
                "gradient_accumulate_every": 1,
                "lr_warmup_steps": 0,
                "lr_scheduler": "cosine",
                "checkpoint_every": 1,
                "val_every": 1,
                "sample_every": 1,
                "rollout_every": 1,
                "use_ema": False,
                "freeze_encoder": False,
                "debug": False,
                "resume": False,
                "tqdm_interval_sec": 0.1
            },
            "ema": {
                "_target_": "diffusion_policy.model.diffusion.ema_model.EMAModel",
                "decay": 0.9999
            },
            "checkpoint": {
                "topk": 5
            },
            "logging": {
                "mode": "disabled",
                "project": "test_project",
                "name": "test_run"
            },
            "head_camera_type": "default",
            "n_obs_steps": 2,
            "n_action_steps": 2,
            "horizon": 4,
            "obs_dim": 10,
            "action_dim": 10
        })
        
        print("âœ… é…ç½®åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºworkspace
        workspace = RobotWorkspace(cfg)
        print("âœ… Workspaceåˆ›å»ºæˆåŠŸ")
        
        return workspace
        
    except Exception as e:
        print(f"âŒ Workspaceåˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_multigpu_integration(fabric, workspace):
    """æµ‹è¯•å¤šå¡è®­ç»ƒé›†æˆ"""
    print("\n=== å¤šå¡è®­ç»ƒé›†æˆæµ‹è¯• ===")
    
    if fabric is None or workspace is None:
        print("âŒ æ— æ³•æµ‹è¯•é›†æˆï¼Œå®ä¾‹åˆ›å»ºå¤±è´¥")
        return False
    
    try:
        # è®¾ç½®å¤šå¡è®­ç»ƒå±æ€§
        workspace.fabric = fabric
        workspace.rank = 0  # æ¨¡æ‹Ÿrank 0
        workspace.world_size = 2  # æ¨¡æ‹Ÿ2å¼ å¡
        
        print("âœ… å¤šå¡è®­ç»ƒå±æ€§è®¾ç½®æˆåŠŸ")
        print(f"   Fabricå®ä¾‹: {workspace.fabric is not None}")
        print(f"   Rank: {workspace.rank}")
        print(f"   World_size: {workspace.world_size}")
        
        # æµ‹è¯•å¤šå¡è®­ç»ƒé€»è¾‘
        if hasattr(workspace, 'fabric') and workspace.fabric is not None:
            print("âœ… å¤šå¡è®­ç»ƒæ¨¡å¼æ£€æµ‹æˆåŠŸ")
            return True
        else:
            print("âŒ å¤šå¡è®­ç»ƒæ¨¡å¼æ£€æµ‹å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ å¤šå¡è®­ç»ƒé›†æˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("=== å®Œæ•´å¤šå¡è®­ç»ƒç¯å¢ƒæµ‹è¯• ===")
    
    # æµ‹è¯•ç¯å¢ƒ
    env_ok = test_environment()
    
    # æµ‹è¯•Lightning
    lightning_ok = test_lightning()
    
    # æµ‹è¯•Fabric
    fabric = test_fabric_creation()
    fabric_ok = fabric is not None
    
    # æµ‹è¯•æ¨¡å—å¯¼å…¥
    imports_ok = test_module_imports()
    
    # æµ‹è¯•Workspaceåˆ›å»º
    workspace = test_workspace_creation()
    workspace_ok = workspace is not None
    
    # æµ‹è¯•å¤šå¡è®­ç»ƒé›†æˆ
    integration_ok = False
    if fabric_ok and workspace_ok:
        integration_ok = test_multigpu_integration(fabric, workspace)
    
    # æ€»ç»“
    print("\n=== æµ‹è¯•æ€»ç»“ ===")
    print(f"ç¯å¢ƒé…ç½®: {'âœ…' if env_ok else 'âŒ'}")
    print(f"LightningåŠŸèƒ½: {'âœ…' if lightning_ok else 'âŒ'}")
    print(f"Fabricåˆ›å»º: {'âœ…' if fabric_ok else 'âŒ'}")
    print(f"æ¨¡å—å¯¼å…¥: {'âœ…' if imports_ok else 'âŒ'}")
    print(f"Workspaceåˆ›å»º: {'âœ…' if workspace_ok else 'âŒ'}")
    print(f"å¤šå¡è®­ç»ƒé›†æˆ: {'âœ…' if integration_ok else 'âŒ'}")
    
    if all([env_ok, lightning_ok, fabric_ok, imports_ok, workspace_ok, integration_ok]):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¤šå¡è®­ç»ƒç¯å¢ƒå®Œå…¨æ­£å¸¸")
        print("ç°åœ¨å¯ä»¥å®‰å…¨åœ°è¿è¡Œè®­ç»ƒå‘½ä»¤:")
        print("bash train_multi_gpu.sh six_tasks demo_clean 1200 0 3 \"0,1,2\"")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥é…ç½®")
        
        if not fabric_ok:
            print("å»ºè®®: æ£€æŸ¥Lightningç‰ˆæœ¬å’ŒCUDAç¯å¢ƒ")
        if not imports_ok:
            print("å»ºè®®: æ£€æŸ¥Pythonè·¯å¾„è®¾ç½®")
        if not workspace_ok:
            print("å»ºè®®: æ£€æŸ¥é…ç½®æ–‡ä»¶ç»“æ„")
        if not integration_ok:
            print("å»ºè®®: æ£€æŸ¥å¤šå¡è®­ç»ƒé…ç½®")

if __name__ == "__main__":
    main()
