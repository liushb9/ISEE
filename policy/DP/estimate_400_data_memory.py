#!/usr/bin/env python3
"""
400æ¡æ•°æ®è®­ç»ƒå†…å­˜éœ€æ±‚ä¼°ç®—
"""

import os
import sys
import subprocess

def get_gpu_info():
    """è·å–GPUä¿¡æ¯"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total,memory.used,memory.free', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        gpu_info = []
        for line in result.stdout.strip().split('\n'):
            if line:
                total, used, free = map(int, line.split(', '))
                gpu_info.append({
                    'total': total,
                    'used': used,
                    'free': free
                })
        return gpu_info
    except Exception as e:
        print(f"âš ï¸  æ— æ³•è·å–GPUä¿¡æ¯: {e}")
        return None

def estimate_400_data_memory():
    """ä¼°ç®—400æ¡æ•°æ®è®­ç»ƒçš„å†…å­˜éœ€æ±‚"""
    
    print("=== 400æ¡æ•°æ®è®­ç»ƒå†…å­˜éœ€æ±‚ä¼°ç®— ===")
    
    # è®­ç»ƒå‚æ•°
    expert_data_num = 400
    batch_size = 128  # ä»é…ç½®æ–‡ä»¶è¯»å–
    sequence_length = 8  # horizon from config
    n_obs_steps = 3
    n_action_steps = 6
    action_dim = 10
    obs_shape = [3, 256, 256]  # RGBå›¾åƒ
    
    print(f"ğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"   æ•°æ®æ¡æ•°: {expert_data_num}")
    print(f"   Batch Size: {batch_size}")
    print(f"   åºåˆ—é•¿åº¦: {sequence_length}")
    print(f"   è§‚æµ‹æ­¥æ•°: {n_obs_steps}")
    print(f"   åŠ¨ä½œæ­¥æ•°: {n_action_steps}")
    print(f"   åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"   å›¾åƒå°ºå¯¸: {obs_shape}")
    
    # è®¡ç®—æ•°æ®å¤§å°
    print(f"\nğŸ“Š æ•°æ®å¤§å°è®¡ç®—:")
    
    # å•æ¡æ•°æ®å¤§å°
    obs_size_per_step = obs_shape[0] * obs_shape[1] * obs_shape[2] * 4  # float32, bytes
    action_size_per_step = action_dim * 4  # float32, bytes
    
    single_episode_obs_size = obs_size_per_step * n_obs_steps  # bytes
    single_episode_action_size = action_size_per_step * n_action_steps  # bytes
    single_episode_size = single_episode_obs_size + single_episode_action_size  # bytes
    
    print(f"   å•æ­¥è§‚æµ‹å¤§å°: {obs_size_per_step / (1024**2):.2f} MB")
    print(f"   å•æ­¥åŠ¨ä½œå¤§å°: {action_size_per_step / (1024**2):.2f} MB")
    print(f"   å•æ¡æ•°æ®å¤§å°: {single_episode_size / (1024**2):.2f} MB")
    
    # 400æ¡æ•°æ®æ€»å¤§å°
    total_data_size_gb = (single_episode_size * expert_data_num) / (1024**3)
    print(f"   400æ¡æ•°æ®æ€»å¤§å°: {total_data_size_gb:.2f} GB")
    
    # å†…å­˜éœ€æ±‚ä¼°ç®—
    print(f"\nğŸ§® å†…å­˜éœ€æ±‚ä¼°ç®—:")
    
    # åŸºç¡€å†…å­˜éœ€æ±‚ï¼ˆGBï¼‰
    base_memory = {
        'model_weights': 2.0,        # ResNet18 + UNet
        'optimizer_states': 4.0,     # AdamWä¼˜åŒ–å™¨çŠ¶æ€
        'gradients': 2.0,            # æ¢¯åº¦
        'activations': 1.0,          # æ¿€æ´»å€¼
        'system_overhead': 0.5       # ç³»ç»Ÿå¼€é”€
    }
    
    # æ•°æ®ç›¸å…³å†…å­˜
    batch_obs_size = batch_size * n_obs_steps * obs_size_per_step / (1024**3)  # GB
    batch_action_size = batch_size * n_action_steps * action_size_per_step / (1024**3)  # GB
    
    data_memory = {
        'batch_obs_data': batch_obs_size,
        'batch_action_data': batch_action_size,
        'cached_data': total_data_size_gb * 0.1,  # å‡è®¾ç¼“å­˜10%çš„æ•°æ®
    }
    
    # è®¡ç®—æ€»å†…å­˜
    total_base = sum(base_memory.values())
    total_data = sum(data_memory.values())
    total_memory = total_base + total_data
    
    print(f"ğŸ“Š å†…å­˜éœ€æ±‚è¯¦æƒ…:")
    print(f"   åŸºç¡€å†…å­˜:")
    for key, value in base_memory.items():
        print(f"     {key}: {value:.1f} GB")
    
    print(f"   æ•°æ®å†…å­˜:")
    for key, value in data_memory.items():
        print(f"     {key}: {value:.1f} GB")
    
    print(f"\nğŸ’¾ æ€»å†…å­˜éœ€æ±‚:")
    print(f"   æ€»å†…å­˜: {total_memory:.1f} GB")
    print(f"   æ¨èæ˜¾å­˜: {total_memory * 1.2:.1f} GB (åŒ…å«20%ç¼“å†²)")
    
    # æ£€æŸ¥GPUçŠ¶æ€
    print(f"\nğŸ” æ£€æŸ¥GPUçŠ¶æ€...")
    gpu_info = get_gpu_info()
    
    if gpu_info:
        print(f"GPUçŠ¶æ€:")
        for i, gpu in enumerate(gpu_info):
            free_gb = gpu['free'] / 1024
            total_gb = gpu['total'] / 1024
            used_gb = gpu['used'] / 1024
            
            status = "âœ… å¯ç”¨" if free_gb >= total_memory else f"âŒ æ˜¾å­˜ä¸è¶³ (éœ€è¦{total_memory:.1f}GB)"
            
            print(f"   GPU {i}: {status}")
            print(f"     æ€»æ˜¾å­˜: {total_gb:.1f} GB")
            print(f"     å·²ç”¨æ˜¾å­˜: {used_gb:.1f} GB")
            print(f"     å¯ç”¨æ˜¾å­˜: {free_gb:.1f} GB")
            
            if free_gb >= total_memory:
                print(f"     âœ… å¯ä»¥è¿è¡Œ400æ¡æ•°æ®è®­ç»ƒ")
            else:
                print(f"     âŒ æ˜¾å­˜ä¸è¶³ï¼Œå»ºè®®:")
                print(f"        - å‡å°batch_sizeåˆ°64æˆ–32")
                print(f"        - ä½¿ç”¨gradient checkpointing")
                print(f"        - ç­‰å¾…å…¶ä»–è¿›ç¨‹é‡Šæ”¾GPU")
    else:
        print(f"âš ï¸  æ— æ³•æ£€æŸ¥GPUçŠ¶æ€")
    
    # ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    print(f"   1. å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥å‡å°batch_size:")
    print(f"      - batch_size=64: å†…å­˜éœ€æ±‚çº¦ {total_memory * 0.7:.1f} GB")
    print(f"      - batch_size=32: å†…å­˜éœ€æ±‚çº¦ {total_memory * 0.5:.1f} GB")
    print(f"   2. ä½¿ç”¨gradient checkpointingå¯ä»¥èŠ‚çœçº¦30%æ˜¾å­˜")
    print(f"   3. 400æ¡æ•°æ®ç›¸å¯¹è¾ƒå°‘ï¼Œè®­ç»ƒæ—¶é—´åº”è¯¥ä¸ä¼šå¤ªé•¿")
    print(f"   4. å»ºè®®ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ")

if __name__ == "__main__":
    estimate_400_data_memory()
